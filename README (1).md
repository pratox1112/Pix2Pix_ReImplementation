# Pix2Pix with VGG19 Perceptual Loss (PyTorch Re-Implementation)

This repository provides a clean, modern re-implementation of **Pix2Pix** using PyTorch, extended with a **VGG19 perceptual loss** to significantly improve texture quality, sharpness, and structural details in the generated images.

This re-implementation is based on the official  
[`pytorch-CycleGAN-and-pix2pix`](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)  
repository, with modifications to the **generator loss** and **network architecture** to incorporate a perceptual feature matching term.

---

## ğŸš€ Key Features
- Full PyTorch implementation of **Pix2Pix**
- **VGG19 Perceptual Loss** integrated into the generator objective
- Improved edge clarity, texture detail, and structure preservation
- Fully compatible with GPU environments (CUDA 12.1)
- Works with PyTorch 2.4.0 and Python 3.11
- Includes training, testing, visualization, and HTML output pages
- Clean and minimal environment setup using `environment.yml`

---

## ğŸ”§ Whatâ€™s New in This Version? (Your Contribution)

### âœ” VGG19 Perceptual Loss Added
A pretrained VGG19 model (ImageNet) is used to extract mid-level feature maps  
(layer `conv3_3`, index 16). The perceptual loss encourages:

- Sharper edges  
- Stronger texture reconstruction  
- More realistic structure  
- Reduced blurring compared to standard L1 loss  

The new generator objective becomes:

\[
\mathcal{L}_G =
\mathcal{L}_{GAN}
+ \lambda_{L1} \mathcal{L}_{L1}
+ \lambda_{perc} \mathcal{L}_{perc}
\]

### Files Modified:
- `models/pix2pix_model.py`  
- `models/networks.py` (added VGG19 feature extractor)

---

## ğŸ“ Folder Structure

```
pytorch-CycleGAN-and-pix2pix/
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ test.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pix2pix_model.py          # Modified to include perceptual loss
â”‚   â”œâ”€â”€ networks.py               # Modified VGG19 extractor added
â”‚   â”œâ”€â”€ base_model.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ aligned_dataset.py        # Paired A|B loader
â”‚   â”œâ”€â”€ base_dataset.py
â”‚
â”œâ”€â”€ options/
â”‚   â”œâ”€â”€ base_options.py
â”‚   â”œâ”€â”€ train_options.py
â”‚   â”œâ”€â”€ test_options.py
â”‚
â”œâ”€â”€ util/
â”‚   â”œâ”€â”€ html.py                   # Training/testing HTML pages
â”‚   â”œâ”€â”€ util.py
â”‚   â”œâ”€â”€ visualizer.py
â”‚
â””â”€â”€ datasets/
    â””â”€â”€ facades/
        â”œâ”€â”€ train/
        â”œâ”€â”€ test/
```

---

## ğŸ› ï¸ Environment Setup

Create the environment:

```bash
conda env create -f environment.yml
conda activate pytorch-img2img
```

Example `environment.yml`:

```yaml
name: pytorch-img2img
channels:
  - pytorch
  - conda-forge
  - nvidia
dependencies:
  - python=3.11
  - pytorch=2.4.0
  - torchvision=0.19.0
  - pytorch-cuda=12.1
  - numpy=1.24.3
  - scikit-image
  - pip
  - pip:
      - dominate>=2.8.0
      - Pillow>=10.0.0
      - wandb>=0.16.0
```

---

## ğŸ“¥ Download Dataset

```bash
bash ./datasets/download_pix2pix_dataset.sh facades
```

Or manually place data in:

```
datasets/facades/train/
datasets/facades/test/
```

---

## ğŸ‹ï¸ Training (with Perceptual Loss)

```bash
python train.py   --dataroot ./datasets/facades   --name facades_pix2pix_vgg   --model pix2pix   --dataset_mode aligned   --direction BtoA   --lambda_perceptual 10
```

Training results appear in:

```
checkpoints/facades_pix2pix_vgg/web/index.html
```

---

## ğŸ” Testing

```bash
python test.py   --dataroot ./datasets/facades   --name facades_pix2pix_vgg   --model pix2pix   --dataset_mode aligned   --direction BtoA
```

Results saved to:

```
results/facades_pix2pix_vgg/test_latest/index.html
```

---

## ğŸ“Š Expected Results

Adding perceptual loss improves:

- Texture detail  
- Structural integrity  
- Sharpness  
- Overall perceptual quality  

---

## ğŸ“œ Citation

If you use this code, cite:

**Original Pix2Pix paper:**  
_Isola et al., â€œImage-to-Image Translation with Conditional Adversarial Networksâ€, CVPR 2017._

**Original Repo:**  
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
